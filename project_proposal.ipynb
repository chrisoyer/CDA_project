{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Members:\n",
    "* Christopher Oyer\n",
    "\n",
    "# Project Title: \n",
    "* Diverse Music recommendations from NonNegative Matrix Factorization \n",
    "\n",
    "# Problem Statement:\n",
    "* I will create a recomendation engine for music based on on non-negative matrix factorization.\n",
    "* I would like recommendations to be \"novel\", not \"you'd like the Beatles, Adele, and Drake.\" \n",
    "* To make the engine adjustable: \n",
    "    * I will include an adjustable tuning parameter at prediction time for rare vs basic, so one doesn't just get stuff one has heard of and hasn't bothered to review.\n",
    "    * use a mixture of tastes, so can pick from several taste clusters. E.g. one's favorite ambient music artist for studying and favorite hip-hop artist for workouts are not related, and it is useful to consider one user's taste to be the combination of several taste 'personas'. \n",
    "    * This intereacts with the data source I am using, which is made up of (user, song, play_count) tuples. The play_count will indicate how much the user likes the artists, so this will bias the system to recommend more-played artist, which will need counteracting.\n",
    "\n",
    "# Data Source\n",
    "I am using http://millionsongdataset.com/tasteprofile/, and aggregating from song counts up to artist counts. I think artist recommendations are more useful, and processing to artist counts will reduce the matrix sizes and be faster to train. \n",
    "\n",
    "# Methodology\n",
    "\n",
    "### Factorization method: NNMF extention. \n",
    "I will be factoring a ratings matrix, $\\textbf{X} \\in \\mathbb{R}^{m \\times n} $  \n",
    "as a feature matrix $\\textbf{W} \\in \\mathbb{R}^{m \\times k}$ and a user tensor $\\textbf{U} \\in \\mathbb{R}^{m \\times n \\times p}  $ \n",
    "The P dimension is a set of several 'personas'. Each user is given several personas, which account for different types of taste they might have. \n",
    "with the factorization defined as\n",
    "\n",
    "* $ \\textbf{X}\\equiv argmax _{p=1,2,...P}\\textbf{U}_{iu}\\textbf{V}_d^{\\intercal} $  \n",
    "i.e, for each user x latent feature combination, choose the persona that maximixes this dot product, then multiply the resulting matrix against the item matrix.\n",
    "\n",
    "\n",
    "\n",
    "### Loss function:\n",
    "* prediction accuracy: l2 of difference between predicted and observed (for those user/item pairs that actually existed)\n",
    "* l2 regularization penalty for latent factors, to try to find denser representation space of latent factors. (may or may not include)\n",
    "* l2 regularization for reccomendations: densify predictions, so heavily predicted items are penalized\n",
    "* l2 regularization for number of latent factors in a user persona, to penalize 'single-persona' user state,\n",
    "* penalty term on inverse of distance between each user's personas, to penalize representations that have similar personas within users: I would like to find diverse personas for each user. Penalty is sum over all distances between user's persona vectors. The penalty is 1/x^2. which is differentiable to -2/(x^3 * sum(norm derivatives)). I am concerned with two issues on this:\n",
    "    * it doesn't go to zero so the analytical solution would be $\\infty$. I am planning to subtract a small hyperparameter term to ensure this maxes out after reaching some size.\n",
    "    * I am not sure I have the right derivative of an inverse of an l2 norm of a sum of l2 norms. If this is intractable I am hoping a good initialization is sufficient.\n",
    "\n",
    "\n",
    "$loss \\equiv \\lambda_{predAcc}\\sum_{i,j\\in X} (\\textbf{X}_{i,j} - \\textbf{W}_{i,j}\\textbf{F}_{i,j})^2 $  \n",
    "        $+ \\lambda_{predDiversity}\\lVert\\sum_u\\textbf{W}_{i,j}\\textbf{F}_{i,j} \\rVert^2_2 $  \n",
    "        $+\\lambda_{personas}\\sum_{p}\\sum_{u}\\lVert U_k \\rVert_2^2 $  \n",
    "        $- \\lambda_{personaDistinction} \\sum_m \\dfrac{1}{\\lVert \\sum_{i\\in p} \\sum_{j\\in p \\;s.t.\\; i \\neq j}  (\\lVert \\textbf{U}_k \\rVert_{2(i)}^2 - \\lVert \\textbf{U}_k \\rVert_{2(j)}^2) \\rVert_{2}^2}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Algorithm with Derivatives:\n",
    "1. Preprocess data, normalize user data (to obviate need for bias term)\n",
    "2. initialize data:\n",
    "    * cluster user x item matrix, assign user p personas, \n",
    "3. ALS:\n",
    "    1. fix \n",
    "\n",
    "\n",
    "### Preprocessing\n",
    "could use counts, with 1 listen slightly negative to large counts = positive, like \n",
    "```python\n",
    "    .assign(log_nos=lambda df: np.log2(np.where(df['numbers']==0, 3, df['numbers'])+.1)-2))\n",
    "```\n",
    "* Rational: many listens means you like something, but one listen means you tried and didn't like. \n",
    "\n",
    "### Initialization\n",
    "* simple copy of user vector\n",
    "* cluster items and assign user vectors to top x clusters they are closest to.\n",
    "\n",
    "### prediction\n",
    "* for prediction (not during training), add a multiplication by a scalar & matrix for 'rarety' of item. \n",
    "    * Rarety would be the overall frequency of an item.  \n",
    "    * The additional term would be, for a prediction vector for user 'u' $\\textbf{p} = \\hat{\\textbf{X}}_u$  and item frequency vector  $\\textbf{freq}$ and item inverse frequency vector $\\textbf{invfreq}_n \\equiv  1/\\textbf{freq}_n$   \n",
    "the prediction would be $\\textbf{p} \\odot (tuningFactor* \\textbf{freq}) \\odot \\textbf{invfreq}$  \n",
    "    * So using the tuning scalar with tuningFactor=1 would be the same; tuningFactor>1 would favor common items, and tuningFactor < 1 would favor rare items.\n",
    "\n",
    "### Training algorithm\n",
    "* Assuming I did the math correctly, I will be using Alternating Least Squares with numpy, which seems to have fast convergence. If there is a problem, I will use SGD, which would probably be best implemented in a neural network.\n",
    "* ALS is defined as iteratively setting $\\textbf{W}$ or $\\textbf{F}$ as a constant, and solving the $\\textbf{X} = \\textbf{W}\\textbf{F}$ equation with an analytical solution, then switching the free and fixed variable, solving again, until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Final Results\n",
    "\n",
    "* I will look at the loss as defined above; the reconstruction loss will be the most important; the other regularization terms will hopefully allows qualitatively good predictions while being at worst neutral in their effect on reconstruction loss. \n",
    "* I will look at the prediction results across as holdout set, both for accuracy, and separately for diversity: as an important goal of this reccommender is to give recommendations that are new, so statistical measure like 'fit exponential distribution to prediction frequencies and favor results with low $\\lambda$ vaule. \n",
    "* Hyperparameter tuning as well as testing actual recommendations will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior work/ references:\n",
    "* https://dl.acm.org/doi/pdf/10.1145/2507157.2507209\n",
    "* https://cseweb.ucsd.edu/~jmcauley/pdfs/cikm15.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c51c7963252c8029cd9bc7df5be393256bd5c2aae20481aadec3ecd24d1431f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
